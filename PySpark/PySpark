{"cells":[{"cell_type":"code","source":["from pyspark import SparkContext\n","from pyspark.sql import SparkSession\n","\n","# Creating SparkContext and SparkSession\n","sc = SparkContext.getOrCreate()\n","spark = SparkSession.builder.appName('PySpark DataFrame From RDD').getOrCreate()\n","\n","# Creating RDD\n","rdd = sc.parallelize([\n","    ('C', 85, 76, 87, 91),\n","    ('B', 85, 76, 87, 91),\n","    (\"A\", 85, 78, 96, 92),\n","    (\"A\", 92, 76, 89, 96)\n","], 4)\n","\n","print(type(rdd))\n","\n","# Converting RDD to DataFrame with schema\n","columns = ['Division', 'English', 'Mathematics', 'Physics', 'Chemistry']\n","marks_df = spark.createDataFrame(rdd, schema=columns)\n","\n","print(type(marks_df))\n","marks_df.printSchema()\n","marks_df.show()\n"],"metadata":{"id":"TxtqKtNGkoLa","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1753945147409,"user_tz":-330,"elapsed":1839,"user":{"displayName":"Ruthravarshan S","userId":"04086528856389545603"}},"outputId":"f567daa7-1780-4b6c-cd65-c1746d8e2672"},"execution_count":72,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pyspark.rdd.RDD'>\n","<class 'pyspark.sql.dataframe.DataFrame'>\n","root\n"," |-- Division: string (nullable = true)\n"," |-- English: long (nullable = true)\n"," |-- Mathematics: long (nullable = true)\n"," |-- Physics: long (nullable = true)\n"," |-- Chemistry: long (nullable = true)\n","\n","+--------+-------+-----------+-------+---------+\n","|Division|English|Mathematics|Physics|Chemistry|\n","+--------+-------+-----------+-------+---------+\n","|       C|     85|         76|     87|       91|\n","|       B|     85|         76|     87|       91|\n","|       A|     85|         78|     96|       92|\n","|       A|     92|         76|     89|       96|\n","+--------+-------+-----------+-------+---------+\n","\n"]}]},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","# Re-using SparkSession\n","spark = SparkSession.builder.appName('Direct DataFrame').getOrCreate()\n","\n","# Creating DataFrame directly\n","df = spark.createDataFrame([\n","    (\"sue\", 32),\n","    (\"li\", 3),\n","    (\"bob\", 75),\n","    (\"heo\", 13)\n","], [\"first_name\", \"age\"])\n","\n","df.show()\n"],"metadata":{"id":"CbYOrV-QkoIj","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1753945147414,"user_tz":-330,"elapsed":4,"user":{"displayName":"Ruthravarshan S","userId":"04086528856389545603"}},"outputId":"f3a4ddf7-2832-4455-859b-cd1a53a8c627"},"execution_count":73,"outputs":[{"output_type":"stream","name":"stdout","text":["+----------+---+\n","|first_name|age|\n","+----------+---+\n","|       sue| 32|\n","|        li|  3|\n","|       bob| 75|\n","|       heo| 13|\n","+----------+---+\n","\n"]}]},{"cell_type":"code","source":["from pyspark.sql.functions import col, when\n","\n","# Adding new column 'life_stage' based on age\n","df1 = df.withColumn(\"life_stage\",\n","    when(col(\"age\") < 13, \"child\")\n","    .when(col(\"age\").between(13, 19), \"teenager\")\n","    .otherwise(\"adult\")\n",")\n","\n","df1.show()"],"metadata":{"id":"PKXxz3bBkqlU","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1753945147463,"user_tz":-330,"elapsed":48,"user":{"displayName":"Ruthravarshan S","userId":"04086528856389545603"}},"outputId":"2fa3ef22-7c2e-478e-dc52-100a5b96ecad"},"execution_count":74,"outputs":[{"output_type":"stream","name":"stdout","text":["+----------+---+----------+\n","|first_name|age|life_stage|\n","+----------+---+----------+\n","|       sue| 32|     adult|\n","|        li|  3|     child|\n","|       bob| 75|     adult|\n","|       heo| 13|  teenager|\n","+----------+---+----------+\n","\n"]}]},{"cell_type":"code","source":["spark = SparkSession.builder \\\n","    .appName(\"Python Spark create RDD example\") \\\n","    .config(\"spark.some.config.option\", \"some-value\") \\\n","    .getOrCreate()\n","\n","Employee = spark.createDataFrame([\n","    (\"1\", 'Joe', '70000', '1'),\n","    (\"2\", 'Henry', '80000', '2'),\n","    (\"3\", 'Sam', '60000', '2'),\n","    (\"4\", 'Max', '90000', '1')\n","], ['Id', 'Name', 'Sallary', 'DepartmentId'])\n","\n","Employee.show()\n"],"metadata":{"id":"qQsQmJWYkurj","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1753945147969,"user_tz":-330,"elapsed":508,"user":{"displayName":"Ruthravarshan S","userId":"04086528856389545603"}},"outputId":"60851039-188f-4af0-e4ff-ff5bcf76a2d7"},"execution_count":75,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+-----+-------+------------+\n","| Id| Name|Sallary|DepartmentId|\n","+---+-----+-------+------------+\n","|  1|  Joe|  70000|           1|\n","|  2|Henry|  80000|           2|\n","|  3|  Sam|  60000|           2|\n","|  4|  Max|  90000|           1|\n","+---+-----+-------+------------+\n","\n"]}]},{"cell_type":"code","source":["# Create Marks_data.csv\n","with open(\"Marks_data.csv\", \"w\") as f:\n","    f.write(\"Division,English,Mathematics,Physics,Chemistry\\n\")\n","    f.write(\"A,85,78,96,92\\n\")\n","    f.write(\"A,92,76,89,96\\n\")\n","    f.write(\"B,85,76,87,91\\n\")\n","    f.write(\"C,85,76,87,91\\n\")\n","\n","# Create example.txt\n","with open(\"example.txt\", \"w\") as f:\n","    f.write(\"This is the first line of the example text file.\\n\")\n","    f.write(\"Spark is used for big data processing.\\n\")\n","    f.write(\"PySpark makes it easy to use Spark with Python.\\n\")\n","    f.write(\"This file is for testing reading text data with Spark.\\n\")\n","\n","# Create example.json\n","with open(\"example.json\", \"w\") as f:\n","    f.write('''[\n","  {\n","    \"id\": 1,\n","    \"name\": \"Ram\",\n","    \"age\": 25,\n","    \"department\": \"HR\"\n","  },\n","  {\n","    \"id\": 2,\n","    \"name\": \"Sam\",\n","    \"age\": 30,\n","    \"department\": \"Engineering\"\n","  },\n","  {\n","    \"id\": 3,\n","    \"name\": \"Tom\",\n","    \"age\": 28,\n","    \"department\": \"Sales\"\n","  }\n","]''')\n"],"metadata":{"id":"tbn_i4o_l2li","executionInfo":{"status":"ok","timestamp":1753945147992,"user_tz":-330,"elapsed":22,"user":{"displayName":"Ruthravarshan S","userId":"04086528856389545603"}}},"execution_count":76,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.appName(\"practice-Hexaware\").getOrCreate()\n","\n","# Reading CSV without schema\n","df = spark.read.csv(\"Marks_data.csv\")\n","df.show()\n","\n","# Reading CSV with header and inferred schema\n","df1 = spark.read.csv(\"Marks_data.csv\", header=True, inferSchema=True)\n","df1.show()\n","\n","# Checking columns\n","print(df1.columns)\n"],"metadata":{"id":"sPTZjnDBkx-L","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1753945149134,"user_tz":-330,"elapsed":1132,"user":{"displayName":"Ruthravarshan S","userId":"04086528856389545603"}},"outputId":"8f7b244a-4bd3-4d3d-e695-27f4502a6190"},"execution_count":77,"outputs":[{"output_type":"stream","name":"stdout","text":["+--------+-------+-----------+-------+---------+\n","|     _c0|    _c1|        _c2|    _c3|      _c4|\n","+--------+-------+-----------+-------+---------+\n","|Division|English|Mathematics|Physics|Chemistry|\n","|       A|     85|         78|     96|       92|\n","|       A|     92|         76|     89|       96|\n","|       B|     85|         76|     87|       91|\n","|       C|     85|         76|     87|       91|\n","+--------+-------+-----------+-------+---------+\n","\n","+--------+-------+-----------+-------+---------+\n","|Division|English|Mathematics|Physics|Chemistry|\n","+--------+-------+-----------+-------+---------+\n","|       A|     85|         78|     96|       92|\n","|       A|     92|         76|     89|       96|\n","|       B|     85|         76|     87|       91|\n","|       C|     85|         76|     87|       91|\n","+--------+-------+-----------+-------+---------+\n","\n","['Division', 'English', 'Mathematics', 'Physics', 'Chemistry']\n"]}]},{"cell_type":"code","source":["# Reading a text file\n","txt_file = spark.read.text(\"example.txt\")\n","txt_file.show()\n","\n","# Reading a JSON file\n","json_df = spark.read.json(\"example.json\", multiLine=True)\n","json_df.printSchema()\n","json_df.show()\n","\n","# Print schema and data types\n","print(\"TXT Schema:\")\n","txt_file.printSchema()\n","\n","print(\"JSON Schema:\")\n","json_df.printSchema()\n","\n","print(\"CSV Schema:\")\n","df1.printSchema()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"-LVgwxSallVK","executionInfo":{"status":"ok","timestamp":1753945149698,"user_tz":-330,"elapsed":552,"user":{"displayName":"Ruthravarshan S","userId":"04086528856389545603"}},"outputId":"46ca9ebf-7346-4faa-95f5-3a3aa41a6657"},"execution_count":78,"outputs":[{"output_type":"stream","name":"stdout","text":["+--------------------+\n","|               value|\n","+--------------------+\n","|This is the first...|\n","|Spark is used for...|\n","|PySpark makes it ...|\n","|This file is for ...|\n","+--------------------+\n","\n","root\n"," |-- age: long (nullable = true)\n"," |-- department: string (nullable = true)\n"," |-- id: long (nullable = true)\n"," |-- name: string (nullable = true)\n","\n","+---+-----------+---+----+\n","|age| department| id|name|\n","+---+-----------+---+----+\n","| 25|         HR|  1| Ram|\n","| 30|Engineering|  2| Sam|\n","| 28|      Sales|  3| Tom|\n","+---+-----------+---+----+\n","\n","TXT Schema:\n","root\n"," |-- value: string (nullable = true)\n","\n","JSON Schema:\n","root\n"," |-- age: long (nullable = true)\n"," |-- department: string (nullable = true)\n"," |-- id: long (nullable = true)\n"," |-- name: string (nullable = true)\n","\n","CSV Schema:\n","root\n"," |-- Division: string (nullable = true)\n"," |-- English: integer (nullable = true)\n"," |-- Mathematics: integer (nullable = true)\n"," |-- Physics: integer (nullable = true)\n"," |-- Chemistry: integer (nullable = true)\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Aw8uDoQk5YYY","executionInfo":{"status":"ok","timestamp":1753945149762,"user_tz":-330,"elapsed":63,"user":{"displayName":"Ruthravarshan S","userId":"04086528856389545603"}}},"execution_count":78,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"2Foo0X2x6cn_","executionInfo":{"status":"ok","timestamp":1753945149763,"user_tz":-330,"elapsed":3,"user":{"displayName":"Ruthravarshan S","userId":"04086528856389545603"}}},"execution_count":78,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","log_data = pd.read_csv(\"Marks_data.csv\")\n","print(log_data)\n","type(log_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":297},"id":"FpOi-5-F6clP","executionInfo":{"status":"ok","timestamp":1753945150057,"user_tz":-330,"elapsed":296,"user":{"displayName":"Ruthravarshan S","userId":"04086528856389545603"}},"outputId":"21554433-3198-4109-ee89-6f83c5718bfb"},"execution_count":79,"outputs":[{"output_type":"stream","name":"stdout","text":["  Division  English  Mathematics  Physics  Chemistry\n","0        A       85           78       96         92\n","1        A       92           76       89         96\n","2        B       85           76       87         91\n","3        C       85           76       87         91\n"]},{"output_type":"execute_result","data":{"text/plain":["pandas.core.frame.DataFrame"],"text/html":["<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n","      pre.function-repr-contents {\n","        overflow-x: auto;\n","        padding: 8px 12px;\n","        max-height: 500px;\n","      }\n","\n","      pre.function-repr-contents.function-repr-contents-collapsed {\n","        cursor: pointer;\n","        max-height: 100px;\n","      }\n","    </style>\n","    <pre style=\"white-space: initial; background:\n","         var(--colab-secondary-surface-color); padding: 8px 12px;\n","         border-bottom: 1px solid var(--colab-border-color);\"><b>pandas.core.frame.DataFrame</b><br/>def __init__(data=None, index: Axes | None=None, columns: Axes | None=None, dtype: Dtype | None=None, copy: bool | None=None) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py</a>Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n","\n","Data structure also contains labeled axes (rows and columns).\n","Arithmetic operations align on both row and column labels. Can be\n","thought of as a dict-like container for Series objects. The primary\n","pandas data structure.\n","\n","Parameters\n","----------\n","data : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n","    Dict can contain Series, arrays, constants, dataclass or list-like objects. If\n","    data is a dict, column order follows insertion-order. If a dict contains Series\n","    which have an index defined, it is aligned by its index. This alignment also\n","    occurs if data is a Series or a DataFrame itself. Alignment is done on\n","    Series/DataFrame inputs.\n","\n","    If data is a list of dicts, column order follows insertion-order.\n","\n","index : Index or array-like\n","    Index to use for resulting frame. Will default to RangeIndex if\n","    no indexing information part of input data and no index provided.\n","columns : Index or array-like\n","    Column labels to use for resulting frame when data does not have them,\n","    defaulting to RangeIndex(0, 1, 2, ..., n). If data contains column labels,\n","    will perform column selection instead.\n","dtype : dtype, default None\n","    Data type to force. Only a single dtype is allowed. If None, infer.\n","copy : bool or None, default None\n","    Copy data from inputs.\n","    For dict data, the default of None behaves like ``copy=True``.  For DataFrame\n","    or 2d ndarray input, the default of None behaves like ``copy=False``.\n","    If data is a dict containing one or more Series (possibly of different dtypes),\n","    ``copy=False`` will ensure that these inputs are not copied.\n","\n","    .. versionchanged:: 1.3.0\n","\n","See Also\n","--------\n","DataFrame.from_records : Constructor from tuples, also record arrays.\n","DataFrame.from_dict : From dicts of Series, arrays, or dicts.\n","read_csv : Read a comma-separated values (csv) file into DataFrame.\n","read_table : Read general delimited file into DataFrame.\n","read_clipboard : Read text from clipboard into DataFrame.\n","\n","Notes\n","-----\n","Please reference the :ref:`User Guide &lt;basics.dataframe&gt;` for more information.\n","\n","Examples\n","--------\n","Constructing DataFrame from a dictionary.\n","\n","&gt;&gt;&gt; d = {&#x27;col1&#x27;: [1, 2], &#x27;col2&#x27;: [3, 4]}\n","&gt;&gt;&gt; df = pd.DataFrame(data=d)\n","&gt;&gt;&gt; df\n","   col1  col2\n","0     1     3\n","1     2     4\n","\n","Notice that the inferred dtype is int64.\n","\n","&gt;&gt;&gt; df.dtypes\n","col1    int64\n","col2    int64\n","dtype: object\n","\n","To enforce a single dtype:\n","\n","&gt;&gt;&gt; df = pd.DataFrame(data=d, dtype=np.int8)\n","&gt;&gt;&gt; df.dtypes\n","col1    int8\n","col2    int8\n","dtype: object\n","\n","Constructing DataFrame from a dictionary including Series:\n","\n","&gt;&gt;&gt; d = {&#x27;col1&#x27;: [0, 1, 2, 3], &#x27;col2&#x27;: pd.Series([2, 3], index=[2, 3])}\n","&gt;&gt;&gt; pd.DataFrame(data=d, index=[0, 1, 2, 3])\n","   col1  col2\n","0     0   NaN\n","1     1   NaN\n","2     2   2.0\n","3     3   3.0\n","\n","Constructing DataFrame from numpy ndarray:\n","\n","&gt;&gt;&gt; df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n","...                    columns=[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;])\n","&gt;&gt;&gt; df2\n","   a  b  c\n","0  1  2  3\n","1  4  5  6\n","2  7  8  9\n","\n","Constructing DataFrame from a numpy ndarray that has labeled columns:\n","\n","&gt;&gt;&gt; data = np.array([(1, 2, 3), (4, 5, 6), (7, 8, 9)],\n","...                 dtype=[(&quot;a&quot;, &quot;i4&quot;), (&quot;b&quot;, &quot;i4&quot;), (&quot;c&quot;, &quot;i4&quot;)])\n","&gt;&gt;&gt; df3 = pd.DataFrame(data, columns=[&#x27;c&#x27;, &#x27;a&#x27;])\n","...\n","&gt;&gt;&gt; df3\n","   c  a\n","0  3  1\n","1  6  4\n","2  9  7\n","\n","Constructing DataFrame from dataclass:\n","\n","&gt;&gt;&gt; from dataclasses import make_dataclass\n","&gt;&gt;&gt; Point = make_dataclass(&quot;Point&quot;, [(&quot;x&quot;, int), (&quot;y&quot;, int)])\n","&gt;&gt;&gt; pd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\n","   x  y\n","0  0  0\n","1  0  3\n","2  2  3\n","\n","Constructing DataFrame from Series/DataFrame:\n","\n","&gt;&gt;&gt; ser = pd.Series([1, 2, 3], index=[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;])\n","&gt;&gt;&gt; df = pd.DataFrame(data=ser, index=[&quot;a&quot;, &quot;c&quot;])\n","&gt;&gt;&gt; df\n","   0\n","a  1\n","c  3\n","\n","&gt;&gt;&gt; df1 = pd.DataFrame([1, 2, 3], index=[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], columns=[&quot;x&quot;])\n","&gt;&gt;&gt; df2 = pd.DataFrame(data=df1, index=[&quot;a&quot;, &quot;c&quot;])\n","&gt;&gt;&gt; df2\n","   x\n","a  1\n","c  3</pre>\n","      <script>\n","      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n","        for (const element of document.querySelectorAll('.filepath')) {\n","          element.style.display = 'block'\n","          element.onclick = (event) => {\n","            event.preventDefault();\n","            event.stopPropagation();\n","            google.colab.files.view(element.textContent, 509);\n","          };\n","        }\n","      }\n","      for (const element of document.querySelectorAll('.function-repr-contents')) {\n","        element.onclick = (event) => {\n","          event.preventDefault();\n","          event.stopPropagation();\n","          element.classList.toggle('function-repr-contents-collapsed');\n","        };\n","      }\n","      </script>\n","      </div>"]},"metadata":{},"execution_count":79}]},{"cell_type":"code","source":["#in order to work with pyspark we need to creare a spark session.\n","import pyspark\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName(\"practice-Hexaware\").getOrCreate()\n","\n","spark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":222},"id":"s3oFVksi6eUX","executionInfo":{"status":"ok","timestamp":1753945150057,"user_tz":-330,"elapsed":5,"user":{"displayName":"Ruthravarshan S","userId":"04086528856389545603"}},"outputId":"4ba1b75b-1618-43c3-cabc-34339e5453ae"},"execution_count":80,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pyspark.sql.session.SparkSession at 0x7f72b8164c10>"],"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://5a6ebd8970a6:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.5.1</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>pyspark-shell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "]},"metadata":{},"execution_count":80}]},{"cell_type":"code","source":["df = spark.read.csv(\"Marks_data.csv\")"],"metadata":{"id":"ceT6gHzh6ine","executionInfo":{"status":"ok","timestamp":1753945150263,"user_tz":-330,"elapsed":208,"user":{"displayName":"Ruthravarshan S","userId":"04086528856389545603"}}},"execution_count":81,"outputs":[]},{"cell_type":"code","source":["df.show() #to show the entire file"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"7RFhItZz6lNe","executionInfo":{"status":"ok","timestamp":1753945150423,"user_tz":-330,"elapsed":148,"user":{"displayName":"Ruthravarshan S","userId":"04086528856389545603"}},"outputId":"fa6fc110-d2a1-4def-db2f-a7713273aa9c"},"execution_count":82,"outputs":[{"output_type":"stream","name":"stdout","text":["+--------+-------+-----------+-------+---------+\n","|     _c0|    _c1|        _c2|    _c3|      _c4|\n","+--------+-------+-----------+-------+---------+\n","|Division|English|Mathematics|Physics|Chemistry|\n","|       A|     85|         78|     96|       92|\n","|       A|     92|         76|     89|       96|\n","|       B|     85|         76|     87|       91|\n","|       C|     85|         76|     87|       91|\n","+--------+-------+-----------+-------+---------+\n","\n"]}]},{"cell_type":"code","source":["#renaming columns\n","df1 = spark.read.csv(\"Marks_data.csv\",header = True,inferSchema = True)\n","df1.show()\n","type(df1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":353},"id":"EK5qKhfr6nSO","executionInfo":{"status":"ok","timestamp":1753945151231,"user_tz":-330,"elapsed":806,"user":{"displayName":"Ruthravarshan S","userId":"04086528856389545603"}},"outputId":"25171812-c8fa-467e-9181-ce2e7393ee63"},"execution_count":83,"outputs":[{"output_type":"stream","name":"stdout","text":["+--------+-------+-----------+-------+---------+\n","|Division|English|Mathematics|Physics|Chemistry|\n","+--------+-------+-----------+-------+---------+\n","|       A|     85|         78|     96|       92|\n","|       A|     92|         76|     89|       96|\n","|       B|     85|         76|     87|       91|\n","|       C|     85|         76|     87|       91|\n","+--------+-------+-----------+-------+---------+\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["pyspark.sql.dataframe.DataFrame"],"text/html":["<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n","      pre.function-repr-contents {\n","        overflow-x: auto;\n","        padding: 8px 12px;\n","        max-height: 500px;\n","      }\n","\n","      pre.function-repr-contents.function-repr-contents-collapsed {\n","        cursor: pointer;\n","        max-height: 100px;\n","      }\n","    </style>\n","    <pre style=\"white-space: initial; background:\n","         var(--colab-secondary-surface-color); padding: 8px 12px;\n","         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.sql.dataframe.DataFrame</b><br/>def __init__(jdf: JavaObject, sql_ctx: Union[&#x27;SQLContext&#x27;, &#x27;SparkSession&#x27;])</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py</a>A distributed collection of data grouped into named columns.\n","\n",".. versionadded:: 1.3.0\n","\n",".. versionchanged:: 3.4.0\n","    Supports Spark Connect.\n","\n","Examples\n","--------\n","A :class:`DataFrame` is equivalent to a relational table in Spark SQL,\n","and can be created using various functions in :class:`SparkSession`:\n","\n","&gt;&gt;&gt; people = spark.createDataFrame([\n","...     {&quot;deptId&quot;: 1, &quot;age&quot;: 40, &quot;name&quot;: &quot;Hyukjin Kwon&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 50},\n","...     {&quot;deptId&quot;: 1, &quot;age&quot;: 50, &quot;name&quot;: &quot;Takuya Ueshin&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 100},\n","...     {&quot;deptId&quot;: 2, &quot;age&quot;: 60, &quot;name&quot;: &quot;Xinrong Meng&quot;, &quot;gender&quot;: &quot;F&quot;, &quot;salary&quot;: 150},\n","...     {&quot;deptId&quot;: 3, &quot;age&quot;: 20, &quot;name&quot;: &quot;Haejoon Lee&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 200}\n","... ])\n","\n","Once created, it can be manipulated using the various domain-specific-language\n","(DSL) functions defined in: :class:`DataFrame`, :class:`Column`.\n","\n","To select a column from the :class:`DataFrame`, use the apply method:\n","\n","&gt;&gt;&gt; age_col = people.age\n","\n","A more concrete example:\n","\n","&gt;&gt;&gt; # To create DataFrame using SparkSession\n","... department = spark.createDataFrame([\n","...     {&quot;id&quot;: 1, &quot;name&quot;: &quot;PySpark&quot;},\n","...     {&quot;id&quot;: 2, &quot;name&quot;: &quot;ML&quot;},\n","...     {&quot;id&quot;: 3, &quot;name&quot;: &quot;Spark SQL&quot;}\n","... ])\n","\n","&gt;&gt;&gt; people.filter(people.age &gt; 30).join(\n","...     department, people.deptId == department.id).groupBy(\n","...     department.name, &quot;gender&quot;).agg({&quot;salary&quot;: &quot;avg&quot;, &quot;age&quot;: &quot;max&quot;}).show()\n","+-------+------+-----------+--------+\n","|   name|gender|avg(salary)|max(age)|\n","+-------+------+-----------+--------+\n","|     ML|     F|      150.0|      60|\n","|PySpark|     M|       75.0|      50|\n","+-------+------+-----------+--------+\n","\n","Notes\n","-----\n","A DataFrame should only be created as described above. It should not be directly\n","created via using the constructor.</pre>\n","      <script>\n","      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n","        for (const element of document.querySelectorAll('.filepath')) {\n","          element.style.display = 'block'\n","          element.onclick = (event) => {\n","            event.preventDefault();\n","            event.stopPropagation();\n","            google.colab.files.view(element.textContent, 80);\n","          };\n","        }\n","      }\n","      for (const element of document.querySelectorAll('.function-repr-contents')) {\n","        element.onclick = (event) => {\n","          event.preventDefault();\n","          event.stopPropagation();\n","          element.classList.toggle('function-repr-contents-collapsed');\n","        };\n","      }\n","      </script>\n","      </div>"]},"metadata":{},"execution_count":83}]},{"cell_type":"code","source":["df1.head(2) # head willl return the top records i.e 2 records as mentioned in the head()."],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"c5JSiefe6ot_","executionInfo":{"status":"ok","timestamp":1753945151251,"user_tz":-330,"elapsed":22,"user":{"displayName":"Ruthravarshan S","userId":"04086528856389545603"}},"outputId":"ab596ba1-6ee9-4361-ef4f-ae325b1e69e0"},"execution_count":84,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Row(Division='A', English=85, Mathematics=78, Physics=96, Chemistry=92),\n"," Row(Division='A', English=92, Mathematics=76, Physics=89, Chemistry=96)]"]},"metadata":{},"execution_count":84}]},{"cell_type":"code","source":["df1.printSchema() #tells you about the types of columns"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"mVn7ohuL6qVe","executionInfo":{"status":"ok","timestamp":1753945151274,"user_tz":-330,"elapsed":12,"user":{"displayName":"Ruthravarshan S","userId":"04086528856389545603"}},"outputId":"bd1e2b63-4a47-4d7f-92dc-92d802143a9a"},"execution_count":85,"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- Division: string (nullable = true)\n"," |-- English: integer (nullable = true)\n"," |-- Mathematics: integer (nullable = true)\n"," |-- Physics: integer (nullable = true)\n"," |-- Chemistry: integer (nullable = true)\n","\n"]}]},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName(\"practice\").getOrCreate()\n"],"metadata":{"id":"Lhl4SAFn6rgI","executionInfo":{"status":"ok","timestamp":1753945151298,"user_tz":-330,"elapsed":23,"user":{"displayName":"Ruthravarshan S","userId":"04086528856389545603"}}},"execution_count":86,"outputs":[]},{"cell_type":"code","source":["## viewing schema\n","df1.printSchema()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"7l4y3ihS6s1Y","executionInfo":{"status":"ok","timestamp":1753945151314,"user_tz":-330,"elapsed":10,"user":{"displayName":"Ruthravarshan S","userId":"04086528856389545603"}},"outputId":"c73969ab-8a52-4fdd-aa3c-b28679f676a0"},"execution_count":87,"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- Division: string (nullable = true)\n"," |-- English: integer (nullable = true)\n"," |-- Mathematics: integer (nullable = true)\n"," |-- Physics: integer (nullable = true)\n"," |-- Chemistry: integer (nullable = true)\n","\n"]}]},{"cell_type":"code","source":["df1.columns"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"9TVN63gG6uro","executionInfo":{"status":"ok","timestamp":1753945151438,"user_tz":-330,"elapsed":44,"user":{"displayName":"Ruthravarshan S","userId":"04086528856389545603"}},"outputId":"b346cb35-0593-4199-83e7-7d77c5b89121"},"execution_count":88,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Division', 'English', 'Mathematics', 'Physics', 'Chemistry']"]},"metadata":{},"execution_count":88}]},{"cell_type":"code","source":["df1.head(3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"N_F5eAAv6wwY","executionInfo":{"status":"ok","timestamp":1753945151459,"user_tz":-330,"elapsed":20,"user":{"displayName":"Ruthravarshan S","userId":"04086528856389545603"}},"outputId":"4b7f21ab-d335-4e32-c4ee-7c6e6cd7d568"},"execution_count":89,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Row(Division='A', English=85, Mathematics=78, Physics=96, Chemistry=92),\n"," Row(Division='A', English=92, Mathematics=76, Physics=89, Chemistry=96),\n"," Row(Division='B', English=85, Mathematics=76, Physics=87, Chemistry=91)]"]},"metadata":{},"execution_count":89}]},{"cell_type":"code","source":["# importing module\n","import pyspark\n","\n","# import lit function\n","from pyspark.sql.functions import lit\n","\n","# importing sparksession from pyspark.sql module\n","from pyspark.sql import SparkSession\n","\n","# creating sparksession and giving an app name\n","spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n","\n","# list  of employee data\n","data = [[\"1\", \"sravan\", \"company 1\"],\n","        [\"2\", \"ojaswi\", \"company 1\"],\n","        [\"3\", \"rohith\", \"company 2\"],\n","        [\"4\", \"sridevi\", \"company 1\"],\n","        [\"5\", \"bobby\", \"company 1\"]]\n","\n","# specify column names\n","columns = ['ID', 'NAME', 'Company']\n","\n","# creating a dataframe from the lists of data\n","df = spark.createDataFrame(data, columns)\n","#adding new columns\n","\n","df.withColumn(\"salary\",lit(30000)).show()\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"xozlJR426x1H","executionInfo":{"status":"ok","timestamp":1753945152081,"user_tz":-330,"elapsed":621,"user":{"displayName":"Ruthravarshan S","userId":"04086528856389545603"}},"outputId":"39b18c27-6264-40c7-e9f9-f72179d299c5"},"execution_count":90,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+-------+---------+------+\n","| ID|   NAME|  Company|salary|\n","+---+-------+---------+------+\n","|  1| sravan|company 1| 30000|\n","|  2| ojaswi|company 1| 30000|\n","|  3| rohith|company 2| 30000|\n","|  4|sridevi|company 1| 30000|\n","|  5|  bobby|company 1| 30000|\n","+---+-------+---------+------+\n","\n"]}]},{"cell_type":"code","source":["import io\n","import pandas as pd\n","pd.read_csv(io.StringIO('''\n","import pyspark\n","from pyspark.sql import SparkSession\n","\"spark = SparkSession.builder.appName(\"\"practice-Hexaware\"\").getOrCreate()\"\n","\"df = spark.read.csv(\"\"Marks_data.csv\"\")\"\n","'''), header=None)\n","df.show() #to show the entire file"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"D46Po8WJ6zrw","executionInfo":{"status":"ok","timestamp":1753945152734,"user_tz":-330,"elapsed":642,"user":{"displayName":"Ruthravarshan S","userId":"04086528856389545603"}},"outputId":"8d6120a9-992f-4e47-e888-2db1b74f76f2"},"execution_count":91,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+-------+---------+\n","| ID|   NAME|  Company|\n","+---+-------+---------+\n","|  1| sravan|company 1|\n","|  2| ojaswi|company 1|\n","|  3| rohith|company 2|\n","|  4|sridevi|company 1|\n","|  5|  bobby|company 1|\n","+---+-------+---------+\n","\n"]}]},{"cell_type":"code","source":["df1 = spark.read.csv(\"Marks_data.csv\",header = True,inferSchema = True)\n","df1.show()\n","type(df1)\n","df.columns"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"SrVBA4ST7Ru0","executionInfo":{"status":"ok","timestamp":1753945153581,"user_tz":-330,"elapsed":846,"user":{"displayName":"Ruthravarshan S","userId":"04086528856389545603"}},"outputId":"e1f0fdea-9e2f-478c-97e5-e792cb738445"},"execution_count":92,"outputs":[{"output_type":"stream","name":"stdout","text":["+--------+-------+-----------+-------+---------+\n","|Division|English|Mathematics|Physics|Chemistry|\n","+--------+-------+-----------+-------+---------+\n","|       A|     85|         78|     96|       92|\n","|       A|     92|         76|     89|       96|\n","|       B|     85|         76|     87|       91|\n","|       C|     85|         76|     87|       91|\n","+--------+-------+-----------+-------+---------+\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["['ID', 'NAME', 'Company']"]},"metadata":{},"execution_count":92}]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Sample data for Fish.csv\n","fish_data = {\n","    'Species': ['Bream', 'Roach', 'Pike', 'Smelt'],\n","    'Weight': [242, 120, 300, 150],\n","    'Length': [25.4, 20.0, 30.0, 18.7],\n","    'Height': [11.52, 12.48, 15.00, 10.12]\n","}\n","\n","# Create DataFrame\n","fish_df = pd.DataFrame(fish_data)\n","\n","# Save to CSV\n","fish_df.to_csv('Fish.csv', index=False)\n","\n","\n","# Sample data for Salary.csv\n","salary_data = {\n","    'ID': [1, 2, 3, 4],\n","    'Name': ['Alice', 'Bob', 'Charlie', 'Diana'],\n","    'Department': ['HR', 'IT', 'Finance', 'Marketing'],\n","    'Salary': [35000, 42000, 50000, 39000]\n","}\n","\n","# Create DataFrame\n","salary_df = pd.DataFrame(salary_data)\n","\n","# Save to CSV\n","salary_df.to_csv('Salary.csv', index=False)\n"],"metadata":{"id":"vavE4WgZ-Ksn","executionInfo":{"status":"ok","timestamp":1753945153607,"user_tz":-330,"elapsed":12,"user":{"displayName":"Ruthravarshan S","userId":"04086528856389545603"}}},"execution_count":93,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName('PySpark DataFrame From External Files').getOrCreate()\n","csv_file = spark.read.csv('Fish.csv', sep = ',', inferSchema = True, header = True)"],"metadata":{"id":"icQoeX5S7byN","executionInfo":{"status":"ok","timestamp":1753945154220,"user_tz":-330,"elapsed":591,"user":{"displayName":"Ruthravarshan S","userId":"04086528856389545603"}}},"execution_count":94,"outputs":[]},{"cell_type":"code","source":["json_file = spark.read.json(\"example.json\", multiLine=True)\n","print(type(csv_file))\n","#print(type(txt_file))\n","#print(type(json_file))\n","csv_file.printSchema()\n","#txt_file.printSchema()\n","#json_file.printSchema()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"VQtJnKqS70Yf","executionInfo":{"status":"ok","timestamp":1753945154496,"user_tz":-330,"elapsed":274,"user":{"displayName":"Ruthravarshan S","userId":"04086528856389545603"}},"outputId":"4eae0fdc-62c6-420c-82b0-0c911424ff98"},"execution_count":95,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pyspark.sql.dataframe.DataFrame'>\n","root\n"," |-- Species: string (nullable = true)\n"," |-- Weight: integer (nullable = true)\n"," |-- Length: double (nullable = true)\n"," |-- Height: double (nullable = true)\n","\n"]}]},{"cell_type":"code","source":["df_pandas = df1.toPandas()"],"metadata":{"id":"hnG9U0qW79q_","executionInfo":{"status":"ok","timestamp":1753945154732,"user_tz":-330,"elapsed":194,"user":{"displayName":"Ruthravarshan S","userId":"04086528856389545603"}}},"execution_count":96,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qbTUX1I7APvd","executionInfo":{"status":"ok","timestamp":1753945154756,"user_tz":-330,"elapsed":9,"user":{"displayName":"Ruthravarshan S","userId":"04086528856389545603"}}},"execution_count":96,"outputs":[]},{"cell_type":"code","source":["files = ['Fish.csv', 'Salary.csv']\n","df = spark.read.csv(files, sep=',', inferSchema=True, header=True)\n","print(type(df))  # Output: <class 'pyspark.sql.dataframe.DataFrame'>"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"hW11zLRd8bob","executionInfo":{"status":"ok","timestamp":1753945155416,"user_tz":-330,"elapsed":651,"user":{"displayName":"Ruthravarshan S","userId":"04086528856389545603"}},"outputId":"8243353d-85e7-4968-f107-affb7dc194dc"},"execution_count":97,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pyspark.sql.dataframe.DataFrame'>\n"]}]},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import lit\n","\n","# Start Spark session\n","spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n","\n","# Sample data\n","data = [\n","    [\"1\", \"sravan\", \"company 1\"],\n","    [\"2\", \"ojaswi\", \"company 1\"],\n","    [\"3\", \"rohith\", \"company 2\"],\n","    [\"4\", \"sridevi\", \"company 1\"],\n","    [\"5\", \"bobby\", \"company 1\"]\n","]\n","columns = ['ID', 'NAME', 'Company']\n","\n","# Create DataFrame\n","df = spark.createDataFrame(data, columns)\n","\n","# Add new column 'salary' with fixed value\n","df.withColumn(\"salary\", lit(30000)).show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"NSaoqBqJ8nob","executionInfo":{"status":"ok","timestamp":1753945155821,"user_tz":-330,"elapsed":407,"user":{"displayName":"Ruthravarshan S","userId":"04086528856389545603"}},"outputId":"a44b5312-bf8c-4da7-f790-50b3d11bb511"},"execution_count":98,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+-------+---------+------+\n","| ID|   NAME|  Company|salary|\n","+---+-------+---------+------+\n","|  1| sravan|company 1| 30000|\n","|  2| ojaswi|company 1| 30000|\n","|  3| rohith|company 2| 30000|\n","|  4|sridevi|company 1| 30000|\n","|  5|  bobby|company 1| 30000|\n","+---+-------+---------+------+\n","\n"]}]},{"cell_type":"code","source":["from pyspark.sql.functions import col\n","\n","# Convert ID to integer and create new salary column\n","df = df.withColumn(\"ID\", col(\"ID\").cast(\"int\"))\n","df.withColumn(\"salary\", col(\"ID\") * 2300).show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"yjFobQ5O8pPM","executionInfo":{"status":"ok","timestamp":1753945156154,"user_tz":-330,"elapsed":333,"user":{"displayName":"Ruthravarshan S","userId":"04086528856389545603"}},"outputId":"1e8656a9-f47f-4ebd-d01b-c72a5b462f9e"},"execution_count":99,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+-------+---------+------+\n","| ID|   NAME|  Company|salary|\n","+---+-------+---------+------+\n","|  1| sravan|company 1|  2300|\n","|  2| ojaswi|company 1|  4600|\n","|  3| rohith|company 2|  6900|\n","|  4|sridevi|company 1|  9200|\n","|  5|  bobby|company 1| 11500|\n","+---+-------+---------+------+\n","\n"]}]},{"cell_type":"code","source":["from pyspark.sql.functions import concat_ws\n","\n","# Add column by joining NAME and Company with a separator\n","df.withColumn(\"Full_Info\", concat_ws(\" - \", \"NAME\", \"Company\")).show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"TdLQgrMV8qhs","executionInfo":{"status":"ok","timestamp":1753945156466,"user_tz":-330,"elapsed":311,"user":{"displayName":"Ruthravarshan S","userId":"04086528856389545603"}},"outputId":"7b98242d-9d4a-4917-842a-67664b15eb69"},"execution_count":100,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+-------+---------+-------------------+\n","| ID|   NAME|  Company|          Full_Info|\n","+---+-------+---------+-------------------+\n","|  1| sravan|company 1| sravan - company 1|\n","|  2| ojaswi|company 1| ojaswi - company 1|\n","|  3| rohith|company 2| rohith - company 2|\n","|  4|sridevi|company 1|sridevi - company 1|\n","|  5|  bobby|company 1|  bobby - company 1|\n","+---+-------+---------+-------------------+\n","\n"]}]},{"cell_type":"code","source":["if 'salary' not in df.columns:\n","    df.withColumn(\"salary\", lit(34000)).show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"zi0JR9AG8rrc","executionInfo":{"status":"ok","timestamp":1753945156922,"user_tz":-330,"elapsed":454,"user":{"displayName":"Ruthravarshan S","userId":"04086528856389545603"}},"outputId":"9490dc87-9586-4354-a407-5702d3298249"},"execution_count":101,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+-------+---------+------+\n","| ID|   NAME|  Company|salary|\n","+---+-------+---------+------+\n","|  1| sravan|company 1| 34000|\n","|  2| ojaswi|company 1| 34000|\n","|  3| rohith|company 2| 34000|\n","|  4|sridevi|company 1| 34000|\n","|  5|  bobby|company 1| 34000|\n","+---+-------+---------+------+\n","\n"]}]},{"cell_type":"code","source":["# Transformation & actions\n","collect_rdd = sc.parallelize([1,2,3,4,5])\n","print(collect_rdd.collect())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t1Pp_Gl08s6N","executionInfo":{"status":"ok","timestamp":1753952068305,"user_tz":-330,"elapsed":57,"user":{"displayName":"Ruthravarshan S","userId":"04086528856389545603"}},"outputId":"7b93fe62-9f08-44aa-e756-46cf883f34b3"},"execution_count":103,"outputs":[{"output_type":"stream","name":"stdout","text":["[1, 2, 3, 4, 5]\n"]}]},{"cell_type":"code","source":["count_rdd = sc.parallelize([1, 2, 3, 4, 5, 5, 6, 7, 8, 9])\n","print(count_rdd.count())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R5BwkCkrsKEx","executionInfo":{"status":"ok","timestamp":1753952676103,"user_tz":-330,"elapsed":1112,"user":{"displayName":"Ruthravarshan S","userId":"04086528856389545603"}},"outputId":"d5df9b93-1539-4add-b4a2-afd9fb623166"},"execution_count":104,"outputs":[{"output_type":"stream","name":"stdout","text":["10\n"]}]},{"cell_type":"code","source":["first_rdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n","print(first_rdd.first())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7jt2Ow3aueNJ","executionInfo":{"status":"ok","timestamp":1753952685316,"user_tz":-330,"elapsed":496,"user":{"displayName":"Ruthravarshan S","userId":"04086528856389545603"}},"outputId":"381f5d9a-ccb4-47bb-94c8-21d42096ab97"},"execution_count":105,"outputs":[{"output_type":"stream","name":"stdout","text":["1\n"]}]},{"cell_type":"code","source":["take_rdd = sc.parallelize([1, 2, 3, 4, 5])\n","print(take_rdd.take(3))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gER6OYomugkp","executionInfo":{"status":"ok","timestamp":1753952694182,"user_tz":-330,"elapsed":361,"user":{"displayName":"Ruthravarshan S","userId":"04086528856389545603"}},"outputId":"16f95566-99f0-4358-c9b6-89d97845d403"},"execution_count":106,"outputs":[{"output_type":"stream","name":"stdout","text":["[1, 2, 3]\n"]}]},{"cell_type":"code","source":["reduce_rdd = sc.parallelize([1, 3, 4, 6])\n","print(reduce_rdd.reduce(lambda x, y: x + y))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cJ1JNuLDuizx","executionInfo":{"status":"ok","timestamp":1753952703467,"user_tz":-330,"elapsed":756,"user":{"displayName":"Ruthravarshan S","userId":"04086528856389545603"}},"outputId":"99aba8be-50b7-4151-da5a-01d83c7bf1ac"},"execution_count":107,"outputs":[{"output_type":"stream","name":"stdout","text":["14\n"]}]},{"cell_type":"code","source":["save_rdd = sc.parallelize([1, 2, 3, 4, 5, 6])\n","save_rdd.saveAsTextFile('file_output')"],"metadata":{"id":"TY6kweGguk-p","executionInfo":{"status":"ok","timestamp":1753952709740,"user_tz":-330,"elapsed":916,"user":{"displayName":"Ruthravarshan S","userId":"04086528856389545603"}}},"execution_count":108,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"hd_q37Djumd5"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"/v2/external/notebooks/intro.ipynb","timestamp":1753933472893}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}