{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Name: **Northwind Traders Database**\n",
        "\n",
        "Kaggle Link: https://www.kaggle.com/datasets/jealousleopard/northwind\n",
        "\n",
        "Why this dataset?\n",
        "\n",
        "- Contains 8 related tables (perfect for joins)\n",
        "- Medium-sized dataset (realistic for practice)\n",
        "- Classic relational database structure\n",
        "- Includes customers, orders, products, employees, etc."
      ],
      "metadata": {
        "id": "sppYscoHrcMc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Tables:\n",
        "Customers - Customer information\n",
        "\n",
        "- `Orders` - Order headers\n",
        "- `OrderDetails` - Order line items\n",
        "- `Products` - Product information\n",
        "- `Employees` - Employee data\n",
        "- `Categories` - Product categories\n",
        "- `Shippers` - Shipping companies"
      ],
      "metadata": {
        "id": "6wvPbcggroPN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Initialize Spark Session and Load Data"
      ],
      "metadata": {
        "id": "ZB9oDewsuRfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"NorthwindAnalysis\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "RjK9z54hrnoU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load all tables\n",
        "customers = spark.read.csv(\"customers.csv\", header=True, inferSchema=True)\n",
        "orders = spark.read.csv(\"orders.csv\", header=True, inferSchema=True)\n",
        "order_details = spark.read.csv(\"order_details.csv\", header=True, inferSchema=True)\n",
        "products = spark.read.csv(\"products.csv\", header=True, inferSchema=True)\n",
        "employees = spark.read.csv(\"employees.csv\", header=True, inferSchema=True)\n",
        "categories = spark.read.csv(\"categories.csv\", header=True, inferSchema=True)\n",
        "shippers = spark.read.csv(\"shippers.csv\", header=True, inferSchema=True)\n",
        "# suppliers = spark.read.csv(\"suppliers.csv\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "jFJw96ZxuAG8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "955a2e32-1745-4a3d-eae4-fe59e6e8536f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/content/customers.csv.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-632331330.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load all tables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcustomers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"customers.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0morders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"orders.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0morder_details\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"order_details.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mproducts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"products.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/content/customers.csv."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformation Examples with Northwind Data"
      ],
      "metadata": {
        "id": "J80yr9ltuUse"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Basic Transformations"
      ],
      "metadata": {
        "id": "CwA1OhIxuZ_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter - German customers\n",
        "german_customers = customers.filter(col(\"Country\") == \"Germany\")\n",
        "\n",
        "# Select - Specific columns\n",
        "customer_contact = customers.select(\"CustomerID\", \"CompanyName\", \"ContactName\")\n",
        "\n",
        "# WithColumn - Calculate order age\n",
        "from pyspark.sql.functions import datediff, current_date\n",
        "orders_with_age = orders.withColumn(\n",
        "    \"OrderAgeDays\",\n",
        "    datediff(current_date(), col(\"OrderDate\"))\n",
        ")\n",
        "\n",
        "# Drop - Remove unnecessary columns\n",
        "products_clean = products.drop(\"QuantityPerUnit\")"
      ],
      "metadata": {
        "id": "9ABlI-M4uOJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aggregations"
      ],
      "metadata": {
        "id": "B_lM9HvvugNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Product sales aggregation\n",
        "product_sales = order_details.groupBy(\"ProductID\") \\\n",
        "    .agg(\n",
        "        sum(\"Quantity\").alias(\"TotalUnitsSold\"),\n",
        "        sum(col(\"Quantity\") * col(\"UnitPrice\")).alias(\"TotalRevenue\"),\n",
        "        avg(\"UnitPrice\").alias(\"AvgUnitPrice\")\n",
        "    )\n",
        "\n",
        "# Employee order count\n",
        "employee_performance = orders.groupBy(\"EmployeeID\") \\\n",
        "    .agg(\n",
        "        count(\"OrderID\").alias(\"OrderCount\"),\n",
        "        min(\"OrderDate\").alias(\"FirstOrderDate\"),\n",
        "        max(\"OrderDate\").alias(\"LastOrderDate\")\n",
        "    )"
      ],
      "metadata": {
        "id": "aQxAXC8hubtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Join Operations"
      ],
      "metadata": {
        "id": "qWoasIImujWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete order information\n",
        "complete_orders = orders.join(order_details, \"OrderID\") \\\n",
        "                      .join(products.withColumnRenamed(\"unitPrice\", \"product_unitPrice\"), \"ProductID\") \\\n",
        "                      .join(customers.withColumnRenamed(\"country\", \"customer_country\"), \"CustomerID\") \\\n",
        "                      .join(employees.withColumnRenamed(\"city\", \"employee_city\").withColumnRenamed(\"country\", \"employee_country\"), \"EmployeeID\")\n",
        "\n",
        "# Products with category names\n",
        "products_with_categories = products.join(categories, \"CategoryID\")\n",
        "\n",
        "# Left join to find unsold products\n",
        "unsold_products = products.join(\n",
        "    order_details,\n",
        "    \"ProductID\",\n",
        "    \"left\"\n",
        ").filter(col(\"OrderID\").isNull())"
      ],
      "metadata": {
        "id": "3sjLvr_yuh6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Window Functions"
      ],
      "metadata": {
        "id": "t73Ji_H3umff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Customer order ranking\n",
        "customer_window = Window.partitionBy(\"CustomerID\").orderBy(col(\"OrderDate\").desc())\n",
        "customer_orders_ranked = orders.withColumn(\n",
        "    \"OrderRank\",\n",
        "    rank().over(customer_window)\n",
        ")\n",
        "\n",
        "# Monthly sales growth\n",
        "monthly_sales = orders.join(order_details, \"OrderID\") \\\n",
        "    .groupBy(month(\"OrderDate\").alias(\"Month\")) \\\n",
        "    .agg(sum(col(\"Quantity\") * col(\"UnitPrice\")).alias(\"MonthlySales\"))\n",
        "\n",
        "sales_window = Window.orderBy(\"Month\")\n",
        "monthly_growth = monthly_sales.withColumn(\n",
        "    \"PrevMonthSales\",\n",
        "    lag(\"MonthlySales\").over(sales_window)\n",
        ").withColumn(\n",
        "    \"GrowthPct\",\n",
        "    (col(\"MonthlySales\") - col(\"PrevMonthSales\")) / col(\"PrevMonthSales\") * 100\n",
        ")"
      ],
      "metadata": {
        "id": "uXdsASRouk93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Action Examples"
      ],
      "metadata": {
        "id": "5_x3mC3ZupyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show results\n",
        "complete_orders.show(5)\n",
        "\n",
        "# Count records\n",
        "print(f\"Total customers: {customers.count()}\")\n",
        "print(f\"Total orders: {orders.count()}\")\n",
        "\n",
        "# Collect top products to driver\n",
        "top_products = product_sales.orderBy(col(\"TotalRevenue\").desc()).take(5)\n",
        "\n",
        "# Save results\n",
        "complete_orders.write.mode(\"overwrite\").parquet(\"output/complete_orders.parquet\")\n",
        "product_sales.write.mode(\"overwrite\").csv(\"output/product_sales.csv\", header=True)"
      ],
      "metadata": {
        "id": "dZOEyLpFuqvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Complex Business Queries"
      ],
      "metadata": {
        "id": "gwg3Bet7uwv4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Top 5 Customers by Revenue"
      ],
      "metadata": {
        "id": "ZiOib2KXu1Qx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_customers = complete_orders.groupBy(\n",
        "    \"CustomerID\", \"CompanyName\"\n",
        ").agg(\n",
        "    sum(col(\"Quantity\") * col(\"UnitPrice\")).alias(\"TotalSpent\"),\n",
        "    countDistinct(\"OrderID\").alias(\"OrderCount\")\n",
        ").orderBy(\n",
        "    col(\"TotalSpent\").desc()\n",
        ").limit(5)"
      ],
      "metadata": {
        "id": "CMrG7-pluspw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Employee Sales Performance"
      ],
      "metadata": {
        "id": "-kTcccZMvXhN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "employee_sales = complete_orders.groupBy(\n",
        "    \"EmployeeID\",\n",
        "    \"employeeName\"\n",
        ").agg(\n",
        "    sum(col(\"Quantity\") * col(\"UnitPrice\")).alias(\"TotalSales\"),\n",
        "    countDistinct(\"OrderID\").alias(\"OrderCount\"),\n",
        "    avg(col(\"Quantity\") * col(\"UnitPrice\")).alias(\"AvgOrderValue\")\n",
        ").orderBy(\n",
        "    col(\"TotalSales\").desc()\n",
        ")"
      ],
      "metadata": {
        "id": "NUyshCZKu2iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Product Category Analysis"
      ],
      "metadata": {
        "id": "4O3jMUTsvYnk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "category_performance = products_with_categories.join(\n",
        "    order_details.withColumnRenamed(\"unitPrice\", \"order_unitPrice\"), \"ProductID\"\n",
        ").groupBy(\n",
        "    \"CategoryID\", \"CategoryName\"\n",
        ").agg(\n",
        "    sum(col(\"Quantity\") * col(\"order_unitPrice\")).alias(\"CategoryRevenue\"),\n",
        "    avg(col(\"order_unitPrice\")).alias(\"AvgProductPrice\"),\n",
        "    countDistinct(\"ProductID\").alias(\"ProductCount\")\n",
        ")"
      ],
      "metadata": {
        "id": "RG-Opa20vaht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KXHQbPytv8Tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fab199b"
      },
      "source": [
        "This notebook demonstrates various PySpark transformations and actions applied to the Northwind Traders database. The goal is to showcase common data manipulation techniques using Spark DataFrames.\n",
        "\n",
        "The dataset tables used include:\n",
        "- `Customers`\n",
        "- `Orders`\n",
        "- `OrderDetails`\n",
        "- `Products`\n",
        "- `Employees`\n",
        "- `Categories`\n",
        "- `Shippers`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0c41c28"
      },
      "source": [
        "### PySpark Transformations\n",
        "\n",
        "Transformations are lazy operations that define the data manipulation logic. They do not execute immediately but build a plan that is executed when an action is called.\n",
        "\n",
        "---\n",
        "\n",
        "#### Filtering Data\n",
        "\n",
        "Filtering selects rows based on a condition. Below is an example of filtering the `customers` DataFrame to get only customers from Germany.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12fe6c37"
      },
      "source": [
        "#### Selecting Columns\n",
        "\n",
        "- Selecting chooses specific columns from a DataFrame. Here, we select the CustomerID, CompanyName, and ContactName from the `customers` DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d021ef4"
      },
      "source": [
        "#### Adding New Columns (`withColumn`)\n",
        "\n",
        "- `withColumn` is used to add a new column to a DataFrame or replace an existing one. In this example, we calculate the age of each order in days.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e671bc4e"
      },
      "source": [
        "#### Dropping Columns (`drop`)\n",
        "\n",
        "- The `drop` transformation removes specified columns from a DataFrame. Here, we remove the `QuantityPerUnit` column from the `products` DataFrame.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5687a61"
      },
      "source": [
        "#### Aggregations (`groupBy` and `agg`)\n",
        "\n",
        "Aggregations are used to group data by one or more columns and then perform aggregate functions (like sum, count, average, min, max) on other columns.\n",
        "\n",
        "Below is an example of calculating total units sold, total revenue, and average unit price for each product.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d5bbc02"
      },
      "source": [
        "Here, we aggregate the `orders` DataFrame to find the order count, first order date, and last order date for each employee.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1baa807"
      },
      "source": [
        "#### Join Operations\n",
        "\n",
        "Joins combine data from two or more DataFrames based on related columns. Different types of joins exist, such as inner, outer, left, and right joins.\n",
        "\n",
        "This example demonstrates joining multiple tables (`orders`, `order_details`, `products`, `customers`, `employees`) to create a comprehensive view of orders."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52e37cc7"
      },
      "source": [
        "This code joins `products` with `categories` to add category names to the product information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15e9bc23"
      },
      "source": [
        "This example uses a left join to identify products that have not been sold by checking for null OrderID values in the joined result.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be23e56f"
      },
      "source": [
        "#### Window Functions\n",
        "\n",
        "Window functions perform calculations across a set of DataFrame rows that are related to the current row. They are used for tasks like ranking, calculating moving averages, and accessing previous or subsequent rows.\n",
        "\n",
        "This example uses a window function to rank orders for each customer based on the order date."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df77e8a8"
      },
      "source": [
        "This code calculates the monthly sales and then uses a window function (`lag`) to determine the previous month's sales and calculate the monthly growth percentage.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6a27f76"
      },
      "source": [
        "### PySpark Actions\n",
        "\n",
        "Actions are operations that trigger the execution of the transformations plan and return a result to the driver program or write data to storage.\n",
        "\n",
        "#### Displaying Results (`show`)\n",
        "\n",
        "- The `show()` action displays the top rows of a DataFrame."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97f0e6e9"
      },
      "source": [
        "#### Counting Records (`count`)\n",
        "\n",
        "- The `count()` action returns the number of rows in a DataFrame."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d8339b9"
      },
      "source": [
        "#### Collecting Data (`collect`, `take`)\n",
        "\n",
        "- Actions like `collect()` and `take()` return data from the DataFrame to the driver program. `collect()` brings all data (use with caution on large datasets), while `take(n)` brings the first `n` rows. Here, we collect the top 5 products by revenue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf12c69c"
      },
      "source": [
        "#### Saving Data (`write`)\n",
        "\n",
        "- The `write` action saves the contents of a DataFrame to various data sources (e.g., Parquet, CSV, JSON). The `mode(\"overwrite\")` option is used here to replace the file if it already exists."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GXTqEshwzDBD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}